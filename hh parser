# 1 Парсер hh
import requests as req
from bs4 import BeautifulSoup
import os
import unidecode
import json


# получаем все урлы с вакансиями
vacancies = []
url = f'https://hh.ru/search/vacancy?no_magic=true&L_save_area=true&text=%D0%A0%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%87%D0%B8%D0%BA+python&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page=50'
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}
r = req.get(url, headers=headers)
soup = BeautifulSoup(r.text, 'lxml')
total_pages = int(soup.findAll(class_='bloko-button')[-4].text)

for p in range(0, total_pages):
    url = f'https://hh.ru/search/vacancy?no_magic=true&L_save_area=true&text=%D0%A0%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%87%D0%B8%D0%BA+python&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page=50?page={p}'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}
    r = req.get(url, headers=headers)
    soup = BeautifulSoup(r.text, 'lxml')
    
    vac = soup.findAll('a', class_='serp-item__title')
    
    for v in vac:
        link = v.get('href').split('?')[0]
        vacancies.append(link)
    
print('get all vacancies')

data = {'data':[]}

# парсим нужные данные
for i in vacancies:
    url = i
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}
    r = req.get(url, headers=headers)
    soup = BeautifulSoup(r.text, 'lxml')
    
    try:
        title = soup.find('div', class_='vacancy-title').find('h1', class_='bloko-header-section-1').text
    except:
        pass
    
    try:
        exp = soup.find(attrs={'data-qa':'vacancy-experience'}).text
    except:
        pass
    
    try:
        salary = str(soup.find(attrs={'data-qa':'vacancy-salary-compensation-type-net'}).text).replace(u'\xa0', ' ')
    except:
        pass
    
    try:
        region = soup.find(attrs={'data-qa':'vacancy-view-raw-address'}).text.split(',')[0]
    except:
        pass
    
    data['data'].append({'title':title,
                        'work experience': exp,
                        'salary': salary, 
                        'region': region})
    
with open("data.json", "w") as file:
    json.dump(data, file, ensure_ascii=False)
